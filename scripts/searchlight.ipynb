{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchlight Analysis\n",
    "Searchlight Analysis is used to sequentially analyze small groups of voxels in the brain in order to identify regions of interest. The technique is designed in the following steps:\n",
    "\n",
    "1. Define a sphere of voxels around a seed voxel\n",
    "2. Extract the time series from each voxel in the sphere\n",
    "3. Concatenate the time series into a feature vector\n",
    "4. Train a classifier on the feature vector and the labels\n",
    "5. Use the classifier to predict the labels of the seed voxel\n",
    "6. Repeat steps 1-5 for all seed voxels\n",
    "7. Aggregate the results across all seed voxels to form a statistical map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image as nli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual sample data before truncation:  (120, 120, 28, 156)\n",
      "audio sample data before truncation:  (120, 120, 28, 200)\n",
      "min timepoints:  156\n",
      "audio sample data after truncation:  (120, 120, 28, 156)\n",
      "visual sample data after truncation:  (120, 120, 28, 156)\n"
     ]
    }
   ],
   "source": [
    "# Set up data\n",
    "clean_data_audio = nib.load('../results/audio/cleaned_data_audio.nii.gz')\n",
    "clean_data_visual = nib.load('../results/visual/cleaned_data_visual.nii.gz')\n",
    "\n",
    "# Resample visual data to match audio data\n",
    "visual_resampled = nli.resample_img(\n",
    "    clean_data_visual,\n",
    "    target_affine = clean_data_audio.affine,\n",
    "    target_shape = clean_data_audio.shape[:3],\n",
    "    interpolation = 'linear' \n",
    ")\n",
    "print(\"visual sample data before truncation: \", visual_resampled.shape)\n",
    "print(\"audio sample data before truncation: \", clean_data_audio.shape)\n",
    "\n",
    "# Convert 4D fMRI to a 2D array (samples per timepoint x features)\n",
    "min_timepoints = min(clean_data_audio.shape[-1], visual_resampled.shape[-1]) # get min z axis\n",
    "print(\"min timepoints: \", min_timepoints)\n",
    "\n",
    "audio_data = clean_data_audio.get_fdata()[...,:min_timepoints] # set length to min z axis\n",
    "visual_data = visual_resampled.get_fdata()[...,:min_timepoints]\n",
    "\n",
    "print(\"audio sample data after truncation: \", audio_data.shape)\n",
    "print(\"visual sample data after truncation: \", visual_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 200 and the array at index 1 has size 156",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# combine datasets to create a single feature matrix and labels\u001b[39;00m\n\u001b[1;32m     15\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((X_audio, X_visual))\n\u001b[0;32m---> 16\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_visual\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# test print\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 200 and the array at index 1 has size 156"
     ]
    }
   ],
   "source": [
    "# Reshape data: (x, y, z, time) -> (time, x*y*z)\n",
    "X_audio = audio_data.reshape(audio_data.shape[-1], -1)\n",
    "X_visual = visual_data.reshape(visual_data.shape[-1], -1)\n",
    "\n",
    "# Create labels (0 for audio, 1 for visual)\n",
    "y_audio = np.zeros(X_audio.shape[0])\n",
    "y_visual = np.ones(X_visual.shape[0])\n",
    "\n",
    "# combine datasets to create a single feature matrix and labels\n",
    "X = np.vstack((X_audio, X_visual))\n",
    "y = np.vstack((y_audio, y_visual))\n",
    "\n",
    "# test print\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Set up Linear SVM classifier and train the new feature matrix\n",
    "\n",
    "\n",
    "# Set up model pipeline\n",
    "pipeline = make_pipeline(StandardScaler(), LinearSVC( random_state = 42, tol = 1e-4, C = 1.0, max_iter = 2000))\n",
    "pipeline = Pipeline(steps = [('standardscaler', StandardScaler()),\n",
    "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
    "\n",
    "# set up features\n",
    "n_voxels = 1000\n",
    "X, y = make_classification(n_features=n_voxels, random_state=42)\n",
    "\n",
    "# Train model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Test print the model score\n",
    "print(pipeline.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
