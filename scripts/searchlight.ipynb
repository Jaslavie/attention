{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchlight Analysis\n",
    "Searchlight Analysis is used to sequentially analyze small groups of voxels in the brain in order to identify regions of interest. The technique is designed in the following steps:\n",
    "\n",
    "1. Define a sphere of voxels around a seed voxel\n",
    "2. Extract the time series from each voxel in the sphere\n",
    "3. Concatenate the time series into a feature vector\n",
    "4. Train a classifier on the feature vector and the labels\n",
    "5. Use the classifier to predict the labels of the seed voxel\n",
    "6. Repeat steps 1-5 for all seed voxels\n",
    "7. Aggregate the results across all seed voxels to form a statistical map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image as nli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_training(clean_data_audio, clean_data_visual):\n",
    "    \"\"\"\n",
    "    Load and resample the audio and visual data\n",
    "    \"\"\"\n",
    "\n",
    "    # Resample visual data to match audio data\n",
    "    visual_resampled = nli.resample_img(\n",
    "        clean_data_visual,\n",
    "        target_affine = clean_data_audio.affine,\n",
    "        target_shape = clean_data_audio.shape[:3],\n",
    "        interpolation = 'linear' \n",
    "    )\n",
    "    print(\"visual sample data before truncation: \", visual_resampled.shape)\n",
    "    print(\"audio sample data before truncation: \", clean_data_audio.shape)\n",
    "\n",
    "    # Convert 4D fMRI to a 2D array (samples per timepoint x features)\n",
    "    min_timepoints = min(clean_data_audio.shape[-1], visual_resampled.shape[-1]) # get min z axis\n",
    "    print(\"min timepoints: \", min_timepoints)\n",
    "\n",
    "    audio_data = clean_data_audio.get_fdata()[...,:min_timepoints] # set length to min z axis\n",
    "    visual_data = visual_resampled.get_fdata()[...,:min_timepoints]\n",
    "\n",
    "    print(\"audio sample data after truncation: \", audio_data.shape)\n",
    "    print(\"visual sample data after truncation: \", visual_data.shape)\n",
    "\n",
    "    return audio_data, visual_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data: (x, y, z, time) -> (time, x*y*z)\n",
    "def reshape_data(audio_data, visual_data):\n",
    "    \"\"\"\n",
    "    Reshape the dataset to the feature matrix and labels to prep for training\n",
    "    \"\"\"\n",
    "    X_audio = audio_data.reshape(audio_data.shape[-1], -1)\n",
    "    X_visual = visual_data.reshape(visual_data.shape[-1], -1)\n",
    "\n",
    "    # Create labels (0 for audio, 1 for visual)\n",
    "    y_audio = np.zeros(X_audio.shape[0])\n",
    "    y_visual = np.ones(X_visual.shape[0])\n",
    "\n",
    "    # combine datasets to create a single feature matrix and labels\n",
    "    X = np.vstack((X_audio, X_visual))\n",
    "    y = np.concatenate((y_audio, y_visual))\n",
    "\n",
    "    # test print\n",
    "    print(\"Feature matrix shape: \", X.shape)\n",
    "    print(\"Labels shape: \", y.shape)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Linear SVM classifier and train the new feature matrix\n",
    "This will classify the data into audio and visual categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model pipeline\n",
    "def train_model(X, y):\n",
    "    \"\"\"\n",
    "    X represents a feature matrix and y represents labels\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('standardscaler', StandardScaler()),\n",
    "        ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
    "\n",
    "    # Train model with feature matrix and labels\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Test print the model score\n",
    "    print(\"Model score: \", pipeline.score(X, y))\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def evaluate_model(pipeline, X, y):\n",
    "    \"\"\"\n",
    "    Data represents a feature matrix and labels of the test set\n",
    "    \"\"\"\n",
    "\n",
    "    print(pipeline.score(X, y))\n",
    "    if pipeline.score(X, y) > 0.5:\n",
    "        print(\"Model is performing better than chance\")\n",
    "    else:\n",
    "        print(\"Model is performing worse than chance\")\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Searchlight Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask voxels\n",
    "def create_brain_mask(clean_data_audio, clean_data_visual):\n",
    "    \"\"\"\n",
    "    create a binary mask of brain activity\n",
    "    \"\"\"\n",
    "    audio_data = clean_data_audio.get_fdata()\n",
    "    visual_data = clean_data_visual.get_fdata()\n",
    "\n",
    "    audio_brain_mask = np.mean(audio_data, axis = 3) > 0\n",
    "    visual_brain_mask = np.mean(visual_data, axis = 3) > 0\n",
    "\n",
    "    # convert to int\n",
    "    audio_brain_mask = audio_brain_mask.astype(int)\n",
    "    visual_brain_mask = visual_brain_mask.astype(int)\n",
    "\n",
    "    print(\"Audio brain mask shape: \", audio_brain_mask.shape)\n",
    "    print(\"Visual brain mask shape: \", visual_brain_mask.shape)\n",
    "\n",
    "    # create a small mask for testing\n",
    "    small_mask_audio = np.zeros(audio_brain_mask.shape)\n",
    "    small_mask_visual = np.zeros(visual_brain_mask.shape)\n",
    "\n",
    "    # set a single voxel to 1 for testing\n",
    "    small_mask_audio[30, 31, 13] = 1\n",
    "    small_mask_visual[30, 31, 13] = 1\n",
    "\n",
    "    return audio_brain_mask, visual_brain_mask, small_mask_audio, small_mask_visual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def create_sphere_mask(x, y, z, radius):\n",
    "    \"\"\"\n",
    "    Create a sphere mask for the current voxel\n",
    "    \"\"\"\n",
    "    return np.sqrt((x - radius)**2 + (y - radius)**2 + (z - radius)**2) <= radius\n",
    "\n",
    "def classify_sphere(clean_data_audio, clean_data_visual, x, y, z, radius, sphere_mask):\n",
    "    \"\"\"\n",
    "    Classify the accuracy of the data in the sphere\n",
    "    \"\"\"\n",
    "    # extract data from the sphere for both conditions\n",
    "    sphere_data_audio = clean_data_audio.get_fdata()[sphere_mask]\n",
    "    sphere_data_visual = clean_data_visual.get_fdata()[sphere_mask]\n",
    "\n",
    "    # retrieve time series for voxels in the sphere\n",
    "    # reshape to the data (timepoints x voxels_in_sphere)\n",
    "    X_audio = sphere_data_audio[\n",
    "        x-radius:x+radius+1,\n",
    "        y-radius:y+radius+1,\n",
    "        z-radius:z+radius+1\n",
    "    ][sphere_mask].T\n",
    "    \n",
    "    X_visual = sphere_data_visual[\n",
    "        x-radius:x+radius+1,\n",
    "        y-radius:y+radius+1,\n",
    "        z-radius:z+radius+1\n",
    "    ][sphere_mask].T\n",
    "    \n",
    "    # Create feature matrix and labels\n",
    "    X = np.vstack((X_audio, X_visual))\n",
    "    y = np.concatenate([\n",
    "        np.zeros(X_audio.shape[0]),  # Audio labels\n",
    "        np.ones(X_visual.shape[0])   # Visual labels\n",
    "    ])\n",
    "    \n",
    "    # Train classifier on this sphere\n",
    "    clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', LinearSVC(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Use cross-validation to get accuracy\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def apply_searchlight(clean_data_audio, clean_data_visual, radius = 3):\n",
    "    \"\"\"\n",
    "    Apply searchlight analysis to the data\n",
    "    \"\"\"\n",
    "    # start timer for runtime\n",
    "    begin_time = time.time()\n",
    "\n",
    "    # initialize accuracy map\n",
    "    accuracy_map_audio = np.zeros(clean_data_audio.shape[:3])\n",
    "    accuracy_map_visual = np.zeros(clean_data_visual.shape[:3])\n",
    "\n",
    "    # iterate through each voxel in the brain mask\n",
    "    for x in range(radius, clean_data_audio.shape[0] - radius): # iterate through x from the radius to the end of the x axis\n",
    "        for y in range(radius, clean_data_audio.shape[1] - radius):\n",
    "            for z in range(radius, clean_data_audio.shape[2] - radius):\n",
    "                # extract sphere data for the current voxel\n",
    "                sphere_mask = create_sphere_mask(x, y, z, radius)\n",
    "\n",
    "                # classify the sphere data\n",
    "                sphere_accuracy = classify_sphere(\n",
    "                    clean_data_audio,\n",
    "                    clean_data_visual,\n",
    "                    x,\n",
    "                    y,\n",
    "                    z,\n",
    "                    radius,\n",
    "                    sphere_mask\n",
    "                )\n",
    "\n",
    "                # update the accuracy map\n",
    "                accuracy_map_audio[x, y, z] = sphere_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual sample data before truncation:  (120, 120, 28, 156)\n",
      "audio sample data before truncation:  (120, 120, 28, 200)\n",
      "min timepoints:  156\n",
      "audio sample data after truncation:  (120, 120, 28, 156)\n",
      "visual sample data after truncation:  (120, 120, 28, 156)\n",
      "Feature matrix shape:  (312, 403200)\n",
      "Labels shape:  (312,)\n",
      "Model score:  1.0\n",
      "1.0\n",
      "Model is performing better than chance\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def classification_pipeline():\n",
    "    \"\"\" \n",
    "    Run the classification pipeline\n",
    "    \"\"\"\n",
    "    # Set up data\n",
    "    clean_data_audio = nib.load('../results/audio/cleaned_data_audio.nii.gz')\n",
    "    clean_data_visual = nib.load('../results/visual/cleaned_data_visual.nii.gz')\n",
    "\n",
    "    # Preprocess data\n",
    "    audio_data, visual_data = prep_data_training(clean_data_audio, clean_data_visual)\n",
    "    X, y = reshape_data(audio_data, visual_data)\n",
    "\n",
    "    # Train model\n",
    "    pipeline = train_model(X, y)\n",
    "    evaluate_model(pipeline, X, y)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "pipeline = classification_pipeline()\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio brain mask shape:  (120, 120, 28)\n",
      "Visual brain mask shape:  (64, 64, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 28 and the array at index 1 has size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     accuracy_map_visual \u001b[38;5;241m=\u001b[39m apply_searchlight(clean_data_visual, clean_data_audio, radius \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_map_audio, accuracy_map_visual\n\u001b[0;32m---> 18\u001b[0m accuracy_map_audio, accuracy_map_visual \u001b[38;5;241m=\u001b[39m \u001b[43msearchlight_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy_map_audio)\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36msearchlight_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m audio_brain_mask, visual_brain_mask, small_mask_audio, small_mask_visual \u001b[38;5;241m=\u001b[39m create_brain_mask(clean_data_audio, clean_data_visual)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply searchlight\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m accuracy_map_audio \u001b[38;5;241m=\u001b[39m \u001b[43mapply_searchlight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_data_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_data_visual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m accuracy_map_visual \u001b[38;5;241m=\u001b[39m apply_searchlight(clean_data_visual, clean_data_audio, radius \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_map_audio, accuracy_map_visual\n",
      "Cell \u001b[0;32mIn[6], line 70\u001b[0m, in \u001b[0;36mapply_searchlight\u001b[0;34m(clean_data_audio, clean_data_visual, radius)\u001b[0m\n\u001b[1;32m     67\u001b[0m sphere_mask \u001b[38;5;241m=\u001b[39m create_sphere_mask(x, y, z, radius)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# classify the sphere data\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m sphere_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_sphere\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_data_audio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_data_visual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mradius\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43msphere_mask\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# update the accuracy map\u001b[39;00m\n\u001b[1;32m     81\u001b[0m accuracy_map_audio[x, y, z] \u001b[38;5;241m=\u001b[39m sphere_accuracy\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mclassify_sphere\u001b[0;34m(clean_data_audio, clean_data_visual, x, y, z, radius, sphere_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m X_visual \u001b[38;5;241m=\u001b[39m sphere_data_visual[\n\u001b[1;32m     27\u001b[0m     x\u001b[38;5;241m-\u001b[39mradius:x\u001b[38;5;241m+\u001b[39mradius\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m     y\u001b[38;5;241m-\u001b[39mradius:y\u001b[38;5;241m+\u001b[39mradius\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     29\u001b[0m     z\u001b[38;5;241m-\u001b[39mradius:z\u001b[38;5;241m+\u001b[39mradius\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m ][sphere_mask]\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Create feature matrix and labels\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_visual\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[1;32m     35\u001b[0m     np\u001b[38;5;241m.\u001b[39mzeros(X_audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),  \u001b[38;5;66;03m# Audio labels\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     np\u001b[38;5;241m.\u001b[39mones(X_visual\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])   \u001b[38;5;66;03m# Visual labels\u001b[39;00m\n\u001b[1;32m     37\u001b[0m ])\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Train classifier on this sphere\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/numpy/core/shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 28 and the array at index 1 has size 32"
     ]
    }
   ],
   "source": [
    "def searchlight_pipeline():\n",
    "    \"\"\"\n",
    "    Run the searchlight pipeline\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    clean_data_audio = nib.load('../results/audio/cleaned_data_audio.nii.gz')\n",
    "    clean_data_visual = nib.load('../results/visual/cleaned_data_visual.nii.gz')\n",
    "\n",
    "    # Create brain masks\n",
    "    audio_brain_mask, visual_brain_mask, small_mask_audio, small_mask_visual = create_brain_mask(clean_data_audio, clean_data_visual)\n",
    "\n",
    "    # Apply searchlight\n",
    "    accuracy_map_audio = apply_searchlight(clean_data_audio, clean_data_visual, radius = 3)\n",
    "    accuracy_map_visual = apply_searchlight(clean_data_visual, clean_data_audio, radius = 3)\n",
    "\n",
    "    return accuracy_map_audio, accuracy_map_visual\n",
    "\n",
    "accuracy_map_audio, accuracy_map_visual = searchlight_pipeline()\n",
    "print(accuracy_map_audio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
