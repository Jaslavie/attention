{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchlight Analysis\n",
    "Searchlight Analysis is used to sequentially analyze small groups of voxels in the brain in order to identify regions of interest. The technique is designed in the following steps:\n",
    "\n",
    "1. Define a sphere of voxels around a seed voxel\n",
    "2. Extract the time series from each voxel in the sphere\n",
    "3. Concatenate the time series into a feature vector\n",
    "4. Train a classifier on the feature vector and the labels\n",
    "5. Use the classifier to predict the labels of the seed voxel\n",
    "6. Repeat steps 1-5 for all seed voxels\n",
    "7. Aggregate the results across all seed voxels to form a statistical map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image as nli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_training(clean_data_audio, clean_data_visual):\n",
    "    \"\"\"\n",
    "    Load and resample the audio and visual data\n",
    "    \"\"\"\n",
    "\n",
    "    # Resample visual data to match audio data\n",
    "    visual_resampled = nli.resample_img(\n",
    "        clean_data_visual,\n",
    "        target_affine = clean_data_audio.affine,\n",
    "        target_shape = clean_data_audio.shape[:3],\n",
    "        interpolation = 'linear' \n",
    "    )\n",
    "    print(\"visual sample data before truncation: \", visual_resampled.shape)\n",
    "    print(\"audio sample data before truncation: \", clean_data_audio.shape)\n",
    "\n",
    "    # Convert 4D fMRI to a 2D array (samples per timepoint x features)\n",
    "    min_timepoints = min(clean_data_audio.shape[-1], visual_resampled.shape[-1]) # get min z axis\n",
    "    print(\"min timepoints: \", min_timepoints)\n",
    "\n",
    "    audio_data = clean_data_audio.get_fdata()[...,:min_timepoints] # set length to min z axis\n",
    "    visual_data = visual_resampled.get_fdata()[...,:min_timepoints]\n",
    "\n",
    "    print(\"audio sample data after truncation: \", audio_data.shape)\n",
    "    print(\"visual sample data after truncation: \", visual_data.shape)\n",
    "\n",
    "    return audio_data, visual_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data: (x, y, z, time) -> (time, x*y*z)\n",
    "def reshape_data(audio_data, visual_data):\n",
    "    \"\"\"\n",
    "    Reshape the dataset to the feature matrix and labels to prep for training\n",
    "    \"\"\"\n",
    "    X_audio = audio_data.reshape(audio_data.shape[-1], -1)\n",
    "    X_visual = visual_data.reshape(visual_data.shape[-1], -1)\n",
    "\n",
    "    # Create labels (0 for audio, 1 for visual)\n",
    "    y_audio = np.zeros(X_audio.shape[0])\n",
    "    y_visual = np.ones(X_visual.shape[0])\n",
    "\n",
    "    # combine datasets to create a single feature matrix and labels\n",
    "    X = np.vstack((X_audio, X_visual))\n",
    "    y = np.concatenate((y_audio, y_visual))\n",
    "\n",
    "    # test print\n",
    "    print(\"Feature matrix shape: \", X.shape)\n",
    "    print(\"Labels shape: \", y.shape)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Linear SVM classifier and train the new feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model pipeline\n",
    "def train_model(X, y):\n",
    "    \"\"\"\n",
    "    X represents a feature matrix and y represents labels\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('standardscaler', StandardScaler()),\n",
    "        ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
    "\n",
    "    # Train model with feature matrix and labels\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Test print the model score\n",
    "    print(\"Model score: \", pipeline.score(X, y))\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def evaluate_model(pipeline, X, y):\n",
    "    \"\"\"\n",
    "    Data represents a feature matrix and labels of the test set\n",
    "    \"\"\"\n",
    "\n",
    "    print(pipeline.score(X, y))\n",
    "    if pipeline.score(X, y) > 0.5:\n",
    "        print(\"Model is performing better than chance\")\n",
    "    else:\n",
    "        print(\"Model is performing worse than chance\")\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up searchlight analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual sample data before truncation:  (120, 120, 28, 156)\n",
      "audio sample data before truncation:  (120, 120, 28, 200)\n",
      "min timepoints:  156\n",
      "audio sample data after truncation:  (120, 120, 28, 156)\n",
      "visual sample data after truncation:  (120, 120, 28, 156)\n",
      "Feature matrix shape:  (312, 403200)\n",
      "Labels shape:  (312,)\n",
      "Model score:  1.0\n",
      "1.0\n",
      "Model is performing better than chance\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def searchlight_analysis():\n",
    "    # Set up data\n",
    "    clean_data_audio = nib.load('../results/audio/cleaned_data_audio.nii.gz')\n",
    "    clean_data_visual = nib.load('../results/visual/cleaned_data_visual.nii.gz')\n",
    "\n",
    "    # Preprocess data\n",
    "    audio_data, visual_data = prep_data_training(clean_data_audio, clean_data_visual)\n",
    "    X, y = reshape_data(audio_data, visual_data)\n",
    "\n",
    "    # Train model\n",
    "    pipeline = train_model(X, y)\n",
    "    evaluate_model(pipeline, X, y)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "pipeline = searchlight_analysis()\n",
    "print(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
