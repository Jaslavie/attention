{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searchlight Analysis\n",
    "Searchlight Analysis is used to sequentially analyze small groups of voxels in the brain in order to identify regions of interest. The technique is designed in the following steps:\n",
    "\n",
    "1. Define a sphere of voxels around a seed voxel\n",
    "2. Extract the time series from each voxel in the sphere\n",
    "3. Concatenate the time series into a feature vector\n",
    "4. Train a classifier on the feature vector and the labels\n",
    "5. Use the classifier to predict the labels of the seed voxel\n",
    "6. Repeat steps 1-5 for all seed voxels\n",
    "7. Aggregate the results across all seed voxels to form a statistical map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn import image as nli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio sample data after truncation:  (120, 120, 28, 156)\n",
      "visual sample data after truncation:  (120, 120, 28, 156)\n"
     ]
    }
   ],
   "source": [
    "def prep_data_training(clean_data_audio, clean_data_visual):\n",
    "    \"\"\"\n",
    "    Load and resample the audio and visual data\n",
    "    \"\"\"\n",
    "\n",
    "    # Resample visual data to match audio data\n",
    "    visual_resampled = nli.resample_img(\n",
    "        clean_data_visual,\n",
    "        target_affine = clean_data_audio.affine,\n",
    "        target_shape = clean_data_audio.shape[:3],\n",
    "        interpolation = 'linear' \n",
    "    )\n",
    "    print(\"visual sample data before truncation: \", visual_resampled.shape)\n",
    "    print(\"audio sample data before truncation: \", clean_data_audio.shape)\n",
    "\n",
    "    # Convert 4D fMRI to a 2D array (samples per timepoint x features)\n",
    "    min_timepoints = min(clean_data_audio.shape[-1], visual_resampled.shape[-1]) # get min z axis\n",
    "    print(\"min timepoints: \", min_timepoints)\n",
    "\n",
    "    audio_data = clean_data_audio.get_fdata()[...,:min_timepoints] # set length to min z axis\n",
    "    visual_data = visual_resampled.get_fdata()[...,:min_timepoints]\n",
    "\n",
    "    return audio_data, visual_data\n",
    "\n",
    "print(\"audio sample data after truncation: \", audio_data.shape)\n",
    "print(\"visual sample data after truncation: \", visual_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data: (x, y, z, time) -> (time, x*y*z)\n",
    "def reshape_data(audio_data, visual_data):\n",
    "    \"\"\"\n",
    "    Reshape the dataset to the feature matrix and labels to prep for training\n",
    "    \"\"\"\n",
    "    X_audio = audio_data.reshape(audio_data.shape[-1], -1)\n",
    "    X_visual = visual_data.reshape(visual_data.shape[-1], -1)\n",
    "\n",
    "    # Create labels (0 for audio, 1 for visual)\n",
    "    y_audio = np.zeros(X_audio.shape[0])\n",
    "    y_visual = np.ones(X_visual.shape[0])\n",
    "\n",
    "    # combine datasets to create a single feature matrix and labels\n",
    "    X = np.vstack((X_audio, X_visual))\n",
    "    y = np.vstack((y_audio, y_visual))\n",
    "\n",
    "    # test print\n",
    "    print(\"Feature matrix shape: \", X.shape)\n",
    "    print(\"Labels shape: \", y.shape)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Linear SVM classifier and train the new feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model pipeline\n",
    "def train_model(X, y):\n",
    "    \"\"\"\n",
    "    X represents a feature matrix and y represents labels\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('standardscaler', StandardScaler()),\n",
    "        ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
    "\n",
    "    # set up features\n",
    "    n_voxels = 403200\n",
    "    X, y = make_classification(n_features=n_voxels, random_state=42)\n",
    "\n",
    "    # Train model with feature matrix and labels\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Test print the model score\n",
    "    print(\"Model score: \", pipeline.score(X, y))\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def evaluate_model(pipeline, X, y):\n",
    "    \"\"\"\n",
    "    Data represents a feature matrix and labels of the test set\n",
    "    \"\"\"\n",
    "\n",
    "    print(pipeline.score(X, y))\n",
    "    if pipeline.score(X, y) > 0.5:\n",
    "        print(\"Model is performing better than chance\")\n",
    "    else:\n",
    "        print(\"Model is performing worse than chance\")\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up searchlight analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visual sample data before truncation:  (120, 120, 28, 156)\n",
      "audio sample data before truncation:  (120, 120, 28, 200)\n",
      "min timepoints:  156\n",
      "Feature matrix shape:  (312, 403200)\n",
      "Labels shape:  (2, 156)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaslavie/Desktop/attention/venv/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score:  1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 312]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m     evaluate_model(pipeline, X, y)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pipeline\n\u001b[0;32m---> 16\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43msearchlight_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(pipeline)\n",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m, in \u001b[0;36msearchlight_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m train_model(X, y)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline\n",
      "Cell \u001b[0;32mIn[39], line 27\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(pipeline, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(pipeline, X, y):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Data represents a feature matrix and labels of the test set\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39mscore(X, y) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is performing better than chance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/sklearn/pipeline.py:1004\u001b[0m, in \u001b[0;36mPipeline.score\u001b[0;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         score_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# metadata routing is enabled.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m process_routing(\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m   1009\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/sklearn/base.py:764\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:103\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 312]"
     ]
    }
   ],
   "source": [
    "def searchlight_analysis():\n",
    "    # Set up data\n",
    "    clean_data_audio = nib.load('../results/audio/cleaned_data_audio.nii.gz')\n",
    "    clean_data_visual = nib.load('../results/visual/cleaned_data_visual.nii.gz')\n",
    "\n",
    "    # Preprocess data\n",
    "    audio_data, visual_data = prep_data_training(clean_data_audio, clean_data_visual)\n",
    "    X, y = reshape_data(audio_data, visual_data)\n",
    "\n",
    "    # Train model\n",
    "    pipeline = train_model(X, y)\n",
    "    evaluate_model(pipeline, X, y)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "pipeline = searchlight_analysis()\n",
    "print(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
