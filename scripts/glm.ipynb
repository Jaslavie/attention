{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement GLM and Contrast Function\n",
    "A generalized linear model is used to find where brain activity changes given different tasks\n",
    "The contrast function compares the brain activity between target and general task.\n",
    "\n",
    "We are interested in the following contrast:\n",
    "- Differentiate brain region activity between target and general task during audio condition\n",
    "- Differentiate brain region activity between target and general task during visual condition\n",
    "- Differentiate brain region activity between audio and visual condition after contrast is applied to both conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import nilearn.masking as masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement glm base model\n",
    "def fit_glm(fmri_img, events_data, tr):\n",
    "    \"\"\" \n",
    "    Fit a glm to the fmri data\n",
    "    \"\"\"\n",
    "    print(\"fMRI data shape: \", fmri_img.shape)\n",
    "    print(\"fMRI data min/max: \", np.min(fmri_img.get_fdata()), np.max(fmri_img.get_fdata()))\n",
    "\n",
    "    model = FirstLevelModel(\n",
    "        t_r=tr,\n",
    "        mask_img = fmri_img,\n",
    "        standardize = True,\n",
    "    )\n",
    "    \n",
    "    glm = model.fit(fmri_img, events = events_data)\n",
    "    return glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize stimuli into target and general task\n",
    "def categorize_stimuli(events_data, data_path):\n",
    "    \"\"\" \n",
    "    Categorize stimuli based on the filename patterns\n",
    "    For visual data: 'inh' = general task, 'sel' = target task\n",
    "    For audio data: NA\n",
    "    events_data: pandas dataframe pulled from the events.tsv file\n",
    "    data_path: path to the visual folder \n",
    "    \"\"\"\n",
    "    if 'visual' in data_path: # if there is a data file with visual in the path\n",
    "        if 'inh' in data_path:\n",
    "            events_data['condition'] = 'general' # inhibition task\n",
    "        elif 'sel' in data_path:\n",
    "            events_data['condition'] = 'target' # selection task\n",
    "\n",
    "    #TODO: Need to add audio data categorization\n",
    "    \n",
    "    return events_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement contrast function\n",
    "def contrast_glm_stimuli(glm, events_data, file_path, contrast_def = {'target': 1, 'general': -1}): \n",
    "    \"\"\" \n",
    "    Apply a contrast to the glm results \n",
    "    \"\"\"\n",
    "    events = categorize_stimuli(events_data, file_path)\n",
    "    contrast = glm.compute_contrast(contrast_def)\n",
    "    return contrast\n",
    "\n",
    "def contrast_glm_conditions(glm, contrast_def = {'audio': 1, 'visual': -1}):\n",
    "    \"\"\" \n",
    "    Apply a contrast to the glm results\n",
    "    \"\"\"\n",
    "    contrast = glm.compute_contrast(contrast_def)\n",
    "    return contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRI 3D data shape:  (64, 64, 32)\n",
      "fMRI data shape:  (64, 64, 32)\n",
      "fMRI data min/max:  -3.631302388384938e-05 3.73113865722329e-05\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Given mask is not made of 2 values: [-3.63130239e-05 -3.18509384e-05 -2.96198957e-05 -2.73888529e-05\n -2.51578102e-05 -2.29267674e-05 -2.06957247e-05 -1.84646820e-05\n -1.62336392e-05 -1.40025965e-05 -1.17715537e-05 -9.54051099e-06\n -7.30946825e-06 -5.07842551e-06 -2.84738277e-06 -6.16340026e-07\n  1.61470271e-06  3.84574546e-06  6.07678820e-06  8.30783094e-06\n  1.05388737e-05  1.27699164e-05  1.50009592e-05  1.72320019e-05\n  1.94630446e-05  2.16940874e-05  2.39251301e-05  2.61561729e-05\n  2.83872156e-05  3.06182583e-05  3.28493011e-05  3.50803438e-05\n  3.73113866e-05]. Cannot interpret as true or false.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m fmri_img \u001b[38;5;241m=\u001b[39m nib\u001b[38;5;241m.\u001b[39mload(fmri_data)\n\u001b[1;32m     34\u001b[0m tr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.5\u001b[39m \u001b[38;5;66;03m# test with visual data\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mrun_glm_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 24\u001b[0m, in \u001b[0;36mrun_glm_contrast\u001b[0;34m(fmri_img, data_path, tr)\u001b[0m\n\u001b[1;32m     21\u001b[0m events_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(events_files[\u001b[38;5;241m0\u001b[39m], sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# fit the glm\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m glm \u001b[38;5;241m=\u001b[39m \u001b[43mfit_glm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri_3d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# apply contrast\u001b[39;00m\n\u001b[1;32m     27\u001b[0m contrast_stimuli \u001b[38;5;241m=\u001b[39m contrast_glm_stimuli(glm, events_data)\n",
      "Cell \u001b[0;32mIn[71], line 15\u001b[0m, in \u001b[0;36mfit_glm\u001b[0;34m(fmri_img, events_data, tr)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfMRI data min/max: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmin(fmri_img\u001b[38;5;241m.\u001b[39mget_fdata()), np\u001b[38;5;241m.\u001b[39mmax(fmri_img\u001b[38;5;241m.\u001b[39mget_fdata()))\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m FirstLevelModel(\n\u001b[1;32m     10\u001b[0m     t_r\u001b[38;5;241m=\u001b[39mtr,\n\u001b[1;32m     11\u001b[0m     mask_img \u001b[38;5;241m=\u001b[39m fmri_img,\n\u001b[1;32m     12\u001b[0m     standardize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m glm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mevents_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m glm\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/nilearn/glm/first_level/first_level.py:602\u001b[0m, in \u001b[0;36mFirstLevelModel.fit\u001b[0;34m(self, run_imgs, events, confounds, sample_masks, design_matrices, bins)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img, NiftiMasker):\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker_ \u001b[38;5;241m=\u001b[39m NiftiMasker(\n\u001b[1;32m    591\u001b[0m         mask_img\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img,\n\u001b[1;32m    592\u001b[0m         smoothing_fwhm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmoothing_fwhm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         memory_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_level,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasker_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_imgs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;66;03m# Make sure masker has been fitted otherwise no attribute mask_img_\u001b[39;00m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img\u001b[38;5;241m.\u001b[39m_check_fitted()\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/nilearn/maskers/nifti_masker.py:482\u001b[0m, in \u001b[0;36mNiftiMasker.fit\u001b[0;34m(self, imgs, y)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img_\u001b[38;5;241m.\u001b[39maffine\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# Load data in memory, while also checking that mask is binary/valid\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m data, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmasking\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_mask_img\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_img_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Infer the number of elements (voxels) in the mask\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_elements_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/nilearn/masking.py:82\u001b[0m, in \u001b[0;36mload_mask_img\u001b[0;34m(mask_img, allow_empty)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground of the mask must be represented with 0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven mask contains: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# If there are more than 2 values, the mask is invalid\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven mask is not made of 2 values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot interpret as true or false.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     87\u001b[0m mask \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mas_ndarray(mask, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mask, mask_img\u001b[38;5;241m.\u001b[39maffine\n",
      "\u001b[0;31mValueError\u001b[0m: Given mask is not made of 2 values: [-3.63130239e-05 -3.18509384e-05 -2.96198957e-05 -2.73888529e-05\n -2.51578102e-05 -2.29267674e-05 -2.06957247e-05 -1.84646820e-05\n -1.62336392e-05 -1.40025965e-05 -1.17715537e-05 -9.54051099e-06\n -7.30946825e-06 -5.07842551e-06 -2.84738277e-06 -6.16340026e-07\n  1.61470271e-06  3.84574546e-06  6.07678820e-06  8.30783094e-06\n  1.05388737e-05  1.27699164e-05  1.50009592e-05  1.72320019e-05\n  1.94630446e-05  2.16940874e-05  2.39251301e-05  2.61561729e-05\n  2.83872156e-05  3.06182583e-05  3.28493011e-05  3.50803438e-05\n  3.73113866e-05]. Cannot interpret as true or false."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from nilearn.image import mean_img\n",
    "# Run the glm and contrast\n",
    "def run_glm_contrast(fmri_img, data_path, tr):\n",
    "    \"\"\" \n",
    "    Run the glm on the fmri data and apply contrast\n",
    "    \"\"\"\n",
    "    fmri_3d = mean_img(fmri_img)\n",
    "    print(\"fMRI 3D data shape: \", fmri_3d.shape)\n",
    "\n",
    "    # get all events files in the events_data_path\n",
    "    pattern = os.path.join(data_path, 'sub-*/func/*events.tsv')\n",
    "    events_files = glob.glob(pattern, recursive = True)\n",
    "\n",
    "    if not events_files:\n",
    "        raise ValueError(f\"No events files found in {pattern}\")\n",
    "    \n",
    "    # read the first events file \n",
    "    events_data = pd.read_csv(events_files[0], sep = '\\t')\n",
    "\n",
    "    # fit the glm\n",
    "    glm = fit_glm(fmri_3d, events_data, tr)\n",
    "\n",
    "    # apply contrast\n",
    "    contrast_stimuli = contrast_glm_stimuli(glm, events_data)\n",
    "    contrast_conditions = contrast_glm_conditions(glm)\n",
    "    return contrast_stimuli, contrast_conditions\n",
    "\n",
    "data_path = \"../data/visual\"\n",
    "fmri_data = \"../results/visual/cleaned_data_visual.nii.gz\" #TODO: need to add audio data\n",
    "fmri_img = nib.load(fmri_data)\n",
    "tr = 1.5 # test with visual data\n",
    "run_glm_contrast(fmri_img, data_path, tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
