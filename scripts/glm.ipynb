{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement GLM and Contrast Function\n",
    "A generalized linear model is used to find where brain activity changes given different tasks\n",
    "The contrast function compares the brain activity between target and general task.\n",
    "\n",
    "We are interested in the following contrast:\n",
    "- Differentiate brain region activity between target and general task during audio condition\n",
    "- Differentiate brain region activity between target and general task during visual condition\n",
    "- Differentiate brain region activity between audio and visual condition after contrast is applied to both conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement glm base model\n",
    "def fit_glm(fmri_data, events_data, tr):\n",
    "    \"\"\" \n",
    "    Fit a glm to the fmri data\n",
    "    \"\"\"\n",
    "    model = FirstLevelModel(t_r = tr)\n",
    "    glm = model.fit(fmri_data, events = events_data)\n",
    "    return glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize stimuli into target and general task\n",
    "def categorize_stimuli(events_data, file_path):\n",
    "    \"\"\" \n",
    "    Categorize stimuli based on the filename patterns\n",
    "    For visual data: 'inh' = general task, 'sel' = target task\n",
    "    For audio data: NA\n",
    "    events_data: pandas dataframe pulled from the events.tsv file\n",
    "    file_path: path to the file\n",
    "    \"\"\"\n",
    "    if 'visual' in file_path: # if there is a data file with visual in the path\n",
    "        if 'inh' in file_path:\n",
    "            events_data['condition'] = 'general' # inhibition task\n",
    "        elif 'sel' in file_path:\n",
    "            events_data['condition'] = 'target' # selection task\n",
    "\n",
    "    #TODO: Need to add audio data categorization\n",
    "    \n",
    "    return events_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement contrast function\n",
    "def contrast_glm_stimuli(glm, events_data, file_path, contrast_def = {'target': 1, 'general': -1}): \n",
    "    \"\"\" \n",
    "    Apply a contrast to the glm results \n",
    "    \"\"\"\n",
    "    events = categorize_stimuli(events_data, file_path)\n",
    "    contrast = glm.compute_contrast(contrast_def)\n",
    "    return contrast\n",
    "\n",
    "def contrast_glm_conditions(glm, contrast_def = {'audio': 1, 'visual': -1}):\n",
    "    \"\"\" \n",
    "    Apply a contrast to the glm results\n",
    "    \"\"\"\n",
    "    contrast = glm.compute_contrast(contrast_def)\n",
    "    return contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: '../results/visual/cleaned_fmri_data.nii.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m fmri_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../results/visual/cleaned_fmri_data.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#TODO: need to add audio data\u001b[39;00m\n\u001b[1;32m     31\u001b[0m tr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.5\u001b[39m \u001b[38;5;66;03m# test with visual data\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mrun_glm_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m, in \u001b[0;36mrun_glm_contrast\u001b[0;34m(fmri_data, data_path, tr, file_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m events_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(events_files[\u001b[38;5;241m0\u001b[39m], sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# fit the glm\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m glm \u001b[38;5;241m=\u001b[39m \u001b[43mfit_glm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# apply contrast\u001b[39;00m\n\u001b[1;32m     24\u001b[0m contrast_stimuli \u001b[38;5;241m=\u001b[39m contrast_glm_stimuli(glm, events_data, file_path)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mfit_glm\u001b[0;34m(fmri_data, events_data, tr)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mFit a glm to the fmri data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m FirstLevelModel(t_r \u001b[38;5;241m=\u001b[39m tr)\n\u001b[0;32m----> 7\u001b[0m glm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mevents_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m glm\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/nilearn/glm/first_level/first_level.py:602\u001b[0m, in \u001b[0;36mFirstLevelModel.fit\u001b[0;34m(self, run_imgs, events, confounds, sample_masks, design_matrices, bins)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img, NiftiMasker):\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker_ \u001b[38;5;241m=\u001b[39m NiftiMasker(\n\u001b[1;32m    591\u001b[0m         mask_img\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img,\n\u001b[1;32m    592\u001b[0m         smoothing_fwhm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmoothing_fwhm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         memory_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_level,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasker_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_imgs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;66;03m# Make sure masker has been fitted otherwise no attribute mask_img_\u001b[39;00m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img\u001b[38;5;241m.\u001b[39m_check_fitted()\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/nilearn/maskers/nifti_masker.py:444\u001b[0m, in \u001b[0;36mNiftiMasker.fit\u001b[0;34m(self, imgs, y)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.fit] Computing the mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmask_args\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img_ \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mcheck_niimg_3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_img)\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/nilearn/masking.py:300\u001b[0m, in \u001b[0;36mcompute_epi_mask\u001b[0;34m(epi_img, lower_cutoff, upper_cutoff, connected, opening, exclude_zeros, ensure_finite, target_affine, target_shape, memory, verbose)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Delayed import to avoid circular imports\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _compute_mean\n\u001b[0;32m--> 300\u001b[0m mean_epi, affine \u001b[38;5;241m=\u001b[39m \u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_compute_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepi_img\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_affine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_affine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43msmooth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopening\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_finite:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# Get rid of memmapping\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     mean_epi \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mas_ndarray(mean_epi)\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/nilearn/image/image.py:463\u001b[0m, in \u001b[0;36m_compute_mean\u001b[0;34m(imgs, target_affine, target_shape, smooth)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resampling\n\u001b[1;32m    461\u001b[0m input_repr \u001b[38;5;241m=\u001b[39m _repr_niimgs(imgs, shorten\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 463\u001b[0m imgs \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_niimg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m mean_data \u001b[38;5;241m=\u001b[39m safe_get_data(imgs)\n\u001b[1;32m    465\u001b[0m affine \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39maffine\n",
      "File \u001b[0;32m~/Desktop/attention/venv/lib/python3.11/site-packages/nilearn/_utils/niimg_conversions.py:300\u001b[0m, in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mniimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(niimg):\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mniimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: File not found: '../results/visual/cleaned_fmri_data.nii.gz'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Run the glm and contrast\n",
    "def run_glm_contrast(fmri_data, data_path, tr, file_path):\n",
    "    \"\"\" \n",
    "    Run the glm on the fmri data and apply contrast\n",
    "    \"\"\"\n",
    "    # get all events files in the events_data_path\n",
    "    pattern = os.path.join(data_path, 'sub-*/func/*events.tsv')\n",
    "    events_files = glob.glob(pattern, recursive = True)\n",
    "\n",
    "    if not events_files:\n",
    "        raise ValueError(f\"No events files found in {pattern}\")\n",
    "    \n",
    "    # read the first events file \n",
    "    events_data = pd.read_csv(events_files[0], sep = '\\t')\n",
    "\n",
    "    # fit the glm\n",
    "    glm = fit_glm(fmri_data, events_data, tr)\n",
    "\n",
    "    # apply contrast\n",
    "    contrast_stimuli = contrast_glm_stimuli(glm, events_data, file_path)\n",
    "    contrast_conditions = contrast_glm_conditions(glm)\n",
    "    return contrast_stimuli, contrast_conditions\n",
    "\n",
    "data_path = \"../data/visual\"\n",
    "file_path = \"../results/visual/cleaned_fmri_data.nii.gz\" \n",
    "fmri_data = \"../results/visual/cleaned_fmri_data.nii.gz\" #TODO: need to add audio data\n",
    "tr = 1.5 # test with visual data\n",
    "run_glm_contrast(fmri_data, data_path, tr, file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
