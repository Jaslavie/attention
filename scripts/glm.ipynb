{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement GLM and Contrast Function\n",
    "A generalized linear model is used to find where brain activity changes given different tasks\n",
    "The contrast function compares the brain activity between target and general task.\n",
    "\n",
    "We are interested in the following contrast:\n",
    "- Differentiate brain region activity between target and general task during audio condition\n",
    "- Differentiate brain region activity between target and general task during visual condition\n",
    "- Differentiate brain region activity between audio and visual condition after contrast is applied to both conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import nilearn.masking as masking\n",
    "from nilearn.image import mean_img\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from nilearn.plotting import plot_design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize stimuli into target and general task\n",
    "def categorize_stimuli(events_data, data_path):\n",
    "    \"\"\" \n",
    "    Categorize stimuli based on the filename patterns in the events.tsv file\n",
    "    For visual data: 'inh' = general task, 'sel' = target task\n",
    "    events_data: pandas dataframe pulled from the events.tsv file\n",
    "    data_path: path to the visual folder \n",
    "    \"\"\"\n",
    "    #extract event file\n",
    "    print(\"base data_path: \", data_path)\n",
    "    print(\"Input data shape: \", events_data.shape)\n",
    "    print(\"Head of input data: \", events_data.head())\n",
    "\n",
    "    pattern = os.path.join(data_path, 'sub-*/func/*events.tsv')\n",
    "    event_files = glob.glob(pattern, recursive=True)\n",
    "    if not event_files:\n",
    "        raise ValueError(f\"No events files found in {pattern}\")\n",
    "\n",
    "    # initialize empty list to store all events\n",
    "    all_events = []\n",
    "\n",
    "    for file_path in event_files:\n",
    "        # load events file\n",
    "        events = pd.read_csv(file_path, sep='\\t')\n",
    "        # categorize stimuli\n",
    "        if 'inh' in file_path:\n",
    "            events['condition'] = 'general'\n",
    "        elif 'sel' in file_path:\n",
    "            events['condition'] = 'target'      \n",
    "        \n",
    "        events['subject'] = os.path.basename(file_path).split('_')[0] # get the id of the subject\n",
    "        all_events.append(events) # append the events to the list\n",
    "    # verify categorization\n",
    "    if events_data['condition'].isnull().all():\n",
    "        raise ValueError(\"No condition column found in events data\")\n",
    "    \n",
    "    # concatenate all events into a single dataframe\n",
    "    categorized_events = pd.concat(all_events, ignore_index=True)\n",
    "\n",
    "    # print summary\n",
    "    print(\"\\nData Summary:\")\n",
    "    print(\"Total number of events:\", len(categorized_events))\n",
    "    print(\"Events by condition:\")\n",
    "    print(categorized_events['condition'].value_counts())\n",
    "    print(\"\\nEvents by subject:\")\n",
    "    print(categorized_events['subject'].value_counts())\n",
    "    \n",
    "    return categorized_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement glm base model\n",
    "def fit_glm(fmri_img, categorized_events, tr):\n",
    "    \"\"\" \n",
    "    Fit a glm to the fmri data and the categorized events data\n",
    "    \"\"\"\n",
    "\n",
    "    mask = masking.compute_epi_mask(\n",
    "        fmri_img,\n",
    "        lower_cutoff = 0.1,\n",
    "        upper_cutoff = 0.9,\n",
    "        connected = False,\n",
    "        opening = False\n",
    "    )\n",
    "\n",
    "    model = FirstLevelModel(\n",
    "        t_r=tr,\n",
    "        mask_img = mask,\n",
    "        standardize = True,\n",
    "        minimize_memory = True,\n",
    "    )\n",
    "    \n",
    "    glm = model.fit(fmri_img, events = categorized_events)\n",
    "    \n",
    "    return glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement contrast function\n",
    "def contrast_glm_stimuli(glm, categorized_events, file_path, contrast_def = {'target': 1, 'general': -1}): \n",
    "    \"\"\" \n",
    "    Apply a contrast to the glm results \n",
    "    \"\"\"\n",
    "    events = categorize_stimuli(categorized_events, file_path)\n",
    "\n",
    "    # convert dictionary to array\n",
    "    conditions = sorted(contrast_def.keys()) # create a list of conditions\n",
    "    contrast_vector = np.array([contrast_def[cond] for cond in conditions])\n",
    "\n",
    "    print(\"Conditions: \", conditions) \n",
    "    print(\"Contrast vector: \", contrast_vector)\n",
    "\n",
    "    contrast = glm.compute_contrast(contrast_vector)\n",
    "    return contrast\n",
    "\n",
    "def contrast_glm_conditions(glm, contrast_def = {'target': 1, 'general': -1}):\n",
    "    \"\"\" \n",
    "    Apply a contrast to the glm results\n",
    "    \"\"\"\n",
    "    conditions = sorted(contrast_def.keys()) # create a list of conditions\n",
    "    contrast_vector = np.array([contrast_def[cond] for cond in conditions])\n",
    "    \n",
    "    contrast = glm.compute_contrast(contrast_vector)\n",
    "    return contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get design matrix\n",
    "def create_design_matrix(glm):\n",
    "    \"\"\" \n",
    "    Create a design matrix for the glm results\n",
    "    \"\"\"\n",
    "     # get design matrix\n",
    "    design_matrix = glm.design_matrices_[0]\n",
    "    # set keys for the design matrix\n",
    "    design_matrix.columns = ['constant', 'target', 'general', 'drift_1', 'drift_2', 'drift_3']\n",
    "    \n",
    "    return design_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters \n",
    "data_path = \"../data/visual\" # path to data folder\n",
    "file_path = \"../data/visual\" # path to all individual data files\n",
    "fmri_data = \"../results/visual/cleaned_data_visual.nii.gz\" #TODO: need to add audio data\n",
    "fmri_img = nib.load(fmri_data)\n",
    "tr = 1.5 # test with visual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base data_path:  ../data/visual\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m glm, design_matrix, contrast_stimuli, contrast_conditions\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Run GLM and get results\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m glm, design_matrix, contrast_stimuli, contrast_conditions \u001b[38;5;241m=\u001b[39m \u001b[43mrun_glm_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Plot design matrix (not the contrast)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m plot_design_matrix(design_matrix)\n",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m, in \u001b[0;36mrun_glm_contrast\u001b[0;34m(fmri_img, data_path, tr)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_glm_contrast\u001b[39m(fmri_img, data_path, tr):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Run the glm on the fmri data and apply contrast\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     categorized_events \u001b[38;5;241m=\u001b[39m \u001b[43mcategorize_stimuli\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Get GLM object \u001b[39;00m\n\u001b[1;32m     10\u001b[0m     glm \u001b[38;5;241m=\u001b[39m fit_glm(fmri_img, categorized_events, tr)\n",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m, in \u001b[0;36mcategorize_stimuli\u001b[0;34m(events_data, data_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#extract event file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase data_path: \u001b[39m\u001b[38;5;124m\"\u001b[39m, data_path)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput data shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mevents_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHead of input data: \u001b[39m\u001b[38;5;124m\"\u001b[39m, events_data\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     14\u001b[0m pattern \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub-*/func/*events.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# function to run contrast and glm\n",
    "def run_glm_contrast(fmri_img, data_path, tr):\n",
    "    \"\"\" \n",
    "    Run the glm on the fmri data and apply contrast\n",
    "    \"\"\"\n",
    "   \n",
    "    categorized_events = categorize_stimuli(None, data_path)\n",
    "    \n",
    "    # Get GLM object \n",
    "    glm = fit_glm(fmri_img, categorized_events, tr)\n",
    "    \n",
    "    # Get design matrix\n",
    "    design_matrix = glm.design_matrices_[0]\n",
    "    design_matrix_columns = design_matrix.columns.tolist()\n",
    "    print(\"columns: \", design_matrix_columns)\n",
    "    design_matrix.columns = ['dummy', 'drift_1', 'drift_2', 'drift_3', 'drift_4', 'constant' ]\n",
    "    \n",
    "    # Compute contrasts\n",
    "    contrast_stimuli = contrast_glm_stimuli(glm, categorized_events, data_path)\n",
    "    contrast_conditions = contrast_glm_conditions(glm)\n",
    "    \n",
    "    return glm, design_matrix, contrast_stimuli, contrast_conditions\n",
    "\n",
    "# Run GLM and get results\n",
    "glm, design_matrix, contrast_stimuli, contrast_conditions = run_glm_contrast(fmri_img, data_path, tr)\n",
    "\n",
    "# Plot design matrix (not the contrast)\n",
    "plot_design_matrix(design_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
